{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-zeeshan/pubmed-abstract-classifier/blob/main/PubMed_Abstract_Formatter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dowloading the Datasets and becoming familiar with the data"
      ],
      "metadata": {
        "id": "UJY1LFekaJhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git"
      ],
      "metadata": {
        "id": "nLelvMufa_tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls pubmed-rct"
      ],
      "metadata": {
        "id": "9I265bcMbJ5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/pubmed-rct/PubMed_200k_RCT/\""
      ],
      "metadata": {
        "id": "i0UzPE9UbcfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing google drive to save model checkpoints and model\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "rHrklUiWI4gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the data"
      ],
      "metadata": {
        "id": "MkkKHNE3bvHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!7z x /content/pubmed-rct/PubMed_200k_RCT/train.7z -o/content/pubmed-rct/PubMed_200k_RCT/"
      ],
      "metadata": {
        "id": "gfvUYl4YONP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/r-zeeshan/pubmed-abstract-classifier/main/helper_functions.py"
      ],
      "metadata": {
        "id": "PeZsWqVIks9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import *"
      ],
      "metadata": {
        "id": "J2UgPZLXqtIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = get_lines(data_dir + \"train.txt\")\n",
        "train_lines[:3]"
      ],
      "metadata": {
        "id": "_cSOlpeHcPDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We need the data in the following format to be able to use it with our model \n",
        "\n",
        "```\n",
        "[{  'line_number' : 0,\n",
        "    'target' : 'BACKGROUND',\n",
        "    'text' : 'Emotional eating is the leading cause of Obesity',\n",
        "    'total_lines' : 11\n",
        "}]\n",
        "```"
      ],
      "metadata": {
        "id": "7wlvApE0cqiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data from the file and preprocess it\n",
        "train_samples = preprocess_text(data_dir + \"train.txt\")\n",
        "test_samples = preprocess_text(data_dir + \"test.txt\")\n",
        "val_samples = preprocess_text(data_dir + \"dev.txt\")\n",
        "\n",
        "print(f\"Length of training samples: {len(train_samples)}\")\n",
        "print(f\"Length of training samples: {len(test_samples)}\")\n",
        "print(f\"Length of training samples: {len(val_samples)}\")"
      ],
      "metadata": {
        "id": "HPqZ3zBNfUR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting our lists into Pandas dataframes\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "\n",
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "jO7xLjAuf3Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting data ready for our model\n",
        "\n",
        "Our model will have 4 different inputs \n",
        "\n",
        "    1. Line Numbers (One Hot Encoded)\n",
        "    2. Total Lines (One Hot Encoded)\n",
        "    3. Train Sentences (Custom Token Embeddings)\n",
        "    4. Train Chars (Custom character embeddings)\n",
        "\n",
        "And output will be target Label which we will also encode\n",
        "\n"
      ],
      "metadata": {
        "id": "LgtOzw2Uhm-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding labels OneHot and LabelEncode\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "\n",
        "print(f\"\\nTrain Labels One Hot Encoded: {train_labels_one_hot}\\n\")\n",
        "print(f\"\\nTrain Labels Encoded: {train_labels_encoded}\\n\")"
      ],
      "metadata": {
        "id": "yGvm0Z-Qind4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_"
      ],
      "metadata": {
        "id": "jyGRpEv3tIgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the text data for our model"
      ],
      "metadata": {
        "id": "RxpZ4I64kSG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connverting out sentences tolist\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()"
      ],
      "metadata": {
        "id": "06VEHL4ElxBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# How long is each sentence on average?\n",
        "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "avg_sent_len  = np.mean(sent_lens)\n",
        "output_seq_len = int(np.percentile(sent_lens, 95))\n",
        "output_seq_len"
      ],
      "metadata": {
        "id": "LfPiFIXUlW2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Word Level Tokenizer and Embedding"
      ],
      "metadata": {
        "id": "iA3qZGaAnet2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Words in our Dataset\n",
        "MAX_TOKENS = 331000 # from the paper\n",
        "\n",
        "# Creating a TextVectorizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens = MAX_TOKENS, output_sequence_length = output_seq_len)"
      ],
      "metadata": {
        "id": "1lBPdg3hkVXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since data is large for Colab RAM, we will convert it into batches\n",
        "train_sentences_dataset = tf.data.Dataset.from_tensor_slices(train_sentences)\n",
        "train_sentences_dataset = train_sentences_dataset.batch(512).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "f_WxDTcANWHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer.adapt(train_sentences_dataset)"
      ],
      "metadata": {
        "id": "ziXQPFyAL5ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words in our training vocabulary?\n",
        "rct_text_vocab = text_vectorizer.get_vocabulary()\n",
        "print(f\"No. of words in vocab: {len(rct_text_vocab)}\")\n",
        "print(f\"Most common words in vocab: {rct_text_vocab[:5]}\")\n",
        "print(f\"Least common words in data: {rct_text_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "BXVrevuamWFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a custom token embedding\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "token_embed = Embedding(input_dim = len(rct_text_vocab),\n",
        "                        output_dim = 512,\n",
        "                        mask_zero = True,\n",
        "                        name = \"token_embeddings\")"
      ],
      "metadata": {
        "id": "7v4hDNkTmjtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a character level tokenizer and embedding"
      ],
      "metadata": {
        "id": "nejftxptnTWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
        "\n",
        "# Average character length in a sentence\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
        "\n",
        "# Getting all the possible chars in the sentences\n",
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
        "char_vectorizer = TextVectorization(max_tokens= NUM_CHAR_TOKENS,\n",
        "                                 output_sequence_length = output_seq_char_len,\n",
        "                                 standardize = None,\n",
        "                                 name = \"char_vectorizer\")"
      ],
      "metadata": {
        "id": "eIRGL95Gnv3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a char dataset using data api \n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices(train_chars)\n",
        "train_char_dataset = train_char_dataset.batch(512).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Svme273TPZs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_vectorizer.adapt(train_char_dataset)\n",
        "char_vocab = char_vectorizer.get_vocabulary()\n",
        "print(f\"No of different characters in character voacb: {len(char_vocab)}\")\n",
        "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
        "print(f\"5 most common characters: {char_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "NP2WLrXpPZHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an embedding layer\n",
        "char_embed = layers.Embedding(input_dim = len(char_vocab),\n",
        "                              output_dim = 25, # this is the size of char_embedding in paper\n",
        "                              mask_zero = True,\n",
        "                              name = \"char_embed\")"
      ],
      "metadata": {
        "id": "-j54NAuTnv0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One hot Encoding the Line Number and Total Lines"
      ],
      "metadata": {
        "id": "PNcgzkT_qspb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tensorflow to create one hot encoded tensors of line number and total lines\n",
        "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
        "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
        "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
        "\n",
        "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)"
      ],
      "metadata": {
        "id": "tbJ67gfbqyTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5TGobTbosCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the fast loading prefetch datasets\n",
        "train_char_token_pos_dataset = create_prefetch_dataset(train_line_numbers_one_hot,\n",
        "                                                       train_total_lines_one_hot,\n",
        "                                                       train_sentences,\n",
        "                                                       train_chars,\n",
        "                                                       train_labels_one_hot,\n",
        "                                                       24)\n",
        "\n",
        "test_char_token_pos_dataset = create_prefetch_dataset(test_line_numbers_one_hot,\n",
        "                                                       test_total_lines_one_hot,\n",
        "                                                       test_sentences,\n",
        "                                                       test_chars,\n",
        "                                                       test_labels_one_hot,\n",
        "                                                       24)\n",
        "\n",
        "val_char_token_pos_dataset = create_prefetch_dataset(val_line_numbers_one_hot,\n",
        "                                                       val_total_lines_one_hot,\n",
        "                                                       val_sentences,\n",
        "                                                       val_chars,\n",
        "                                                       val_labels_one_hot,\n",
        "                                                       24)"
      ],
      "metadata": {
        "id": "5psEohIBreOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Tribrid Model with Custom Token and Word Embeddings"
      ],
      "metadata": {
        "id": "kZ_TLAGgsVeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Concatenate, Dropout\n",
        "\n",
        "# 1. Token Inputs\n",
        "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
        "token_vectors = text_vectorizer(token_inputs)\n",
        "token_embeddings = token_embed(token_vectors)\n",
        "tokens_bi_lstm = Bidirectional(LSTM(32))(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs = token_inputs,\n",
        "                             outputs = tokens_bi_lstm,\n",
        "                             name = \"token_model\")\n",
        "\n",
        "# 2. Character Inputs\n",
        "char_input = Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
        "char_vectors = char_vectorizer(char_input)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = Bidirectional(LSTM(32))(char_embeddings)\n",
        "char_model = Model(inputs = char_input, outputs = char_bi_lstm, name=\"char_model\")\n",
        "\n",
        "# 3. Line Numbers\n",
        "line_numbers_input = Input(shape=(15,), dtype=tf.float32, name=\"line_number_input\")\n",
        "x = Dense(256, activation=\"relu\")(line_numbers_input)\n",
        "line_numbers_model = Model(inputs = line_numbers_input, outputs = x, name=\"line_numbers_model\")\n",
        "\n",
        "# 4. Total Lines\n",
        "total_lines_input = Input(shape=(20,), dtype=tf.float32, name=\"total_lines_input\")\n",
        "y = Dense(256, activation=\"relu\")(total_lines_input)\n",
        "total_lines_model = Model(inputs = total_lines_input, outputs = y, name=\"total_lines_model\")\n",
        "\n",
        "# 5. Concatenate the token and char into hybrid embeddings\n",
        "hybrid = Concatenate(name=\"hybrid_token_char_embeddings\")([token_model.output, char_model.output])\n",
        "z = Dense(256, activation=\"relu\")(hybrid)\n",
        "hybrid_embeddings = Dropout(0.5)(z)\n",
        "\n",
        "# 6. Combine positional and hybrid embeddings\n",
        "tribrid_embeddings = Concatenate(name=\"tribrid_embeddings\")([line_numbers_model.output,\n",
        "                                                             total_lines_model.output,\n",
        "                                                             hybrid_embeddings])\n",
        "\n",
        "# 7. Final Output layer\n",
        "output = Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(tribrid_embeddings)\n",
        "\n",
        "# 8. Putting togathere everything\n",
        "tribrid_model = Model(inputs = [line_numbers_model.input,\n",
        "                                total_lines_model.input,\n",
        "                                token_model.input,\n",
        "                                char_model.input],\n",
        "                      outputs = output,\n",
        "                      name = \"tribrid_model\")"
      ],
      "metadata": {
        "id": "69j4x6318-Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(tribrid_model, show_shapes=True)"
      ],
      "metadata": {
        "id": "8RRlohAU4je1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Callbacks"
      ],
      "metadata": {
        "id": "VVINWHlzBsZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/PUBMED/tribrid_checkpoints/\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor=\"val_loss\")\n",
        "\n",
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down \n",
        "                                                 min_lr=1e-7)"
      ],
      "metadata": {
        "id": "xrNgQiGwIPnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tribrid_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                      metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "b3li3jZsIbNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tribrid_model_history = tribrid_model.fit(train_char_token_pos_dataset,\n",
        "                                          steps_per_epoch = int(0.3 * len(train_char_token_pos_dataset)),\n",
        "                                          epochs=20,\n",
        "                                          validation_data = val_char_token_pos_dataset,\n",
        "                                          validation_steps = int(0.3 * len(val_char_token_pos_dataset)),\n",
        "                                          callbacks=[early_stopping, model_checkpoint, reduce_lr])"
      ],
      "metadata": {
        "id": "_JiYXqBL0f0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tribrid_model.save(\"/content/gdrive/MyDrive/PUBMED/tribrid.h5\")"
      ],
      "metadata": {
        "id": "6NK1PX2OSEg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tribrid_model.evaluate(val_char_token_pos_dataset)"
      ],
      "metadata": {
        "id": "fzqM5AcOGR6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tribrid_model_pred_probs = tribrid_model.predict(test_char_token_pos_dataset)\n",
        "tribrid_model_preds = tf.argmax(tribrid_model_pred_probs, axis=1)"
      ],
      "metadata": {
        "id": "2Nzhfq0vGf_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tribrid_model_results = calculate_results(y_true = test_labels_encoded,\n",
        "                                          y_pred = tribrid_model_preds) "
      ],
      "metadata": {
        "id": "mZH_wyJRGr3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tribrid_model_results"
      ],
      "metadata": {
        "id": "IUNn2q4rHjWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLKmKMd8Hmgc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}